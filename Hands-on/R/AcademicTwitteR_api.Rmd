---
title: "Accessing Twitter API using R"
description: |  
      Workshop "Introduction to Social Media and Big Data for Migration Studies"
date: March 2022
author:
  - name: "Jisu Kim"
    url: https://github.com/jisukimmmm
    affiliation: Max Planck Institute for Demographic Research (MPIDR)
output: 
  html_notebook:
    toc: yes
    toc_depth: 3
---


# PAA 2022 Workshop "Introduction to Social Media and Big Data for Migration Studies"

This notebook shows how you can access Twitter API using R. 
Here, we will use "rtweet" and academictwitteR" to show how you can access both version 1 and 2 of Twitter API. 

For more details, please check out also the package descriptions here:<br />
                  - https://cran.r-project.org/web/packages/rtweet/rtweet.pdf <br />
                  - https://rdrr.io/cran/academictwitteR/f/vignettes/academictwitteR-build.Rmd
Here we show how to use academictwitteR package to access version 2 of Twitter API.


## AcademictwitterR
### Load packages 

Load packages first!

```{r Load packages, warning=TRUE}
if(!require(tidyverse)){install.packages("tidyverse"); library(tidyverse)}
# Access the Twitter Academic Research Product Track V2 API Endpoint
# devtools::install_github("cjbarrie/academictwitteR", build_vignettes = TRUE)
if(!require(academictwitteR)){install.packages("academictwitteR"); library(academictwitteR)} 
if(!require(devtools)){install.packages("devtools"); library(devtools)} 
```


Before you begin, we need to set the bearer token:
<Instructions:ℹ 1. Add line: TWITTER_BEARER=YOURTOKENHERE to .Renviron on new line, replacing YOURTOKENHERE with  actual bearer tokenℹ 2. Restart R> 
```{r}
# set path to the folder
path_dir="~dir"

# add token to .Renviron, restart
set_bearer() 

# check that your token is set properly
get_bearer() 
```

### Set query
Once the bearer token is set, we can start downloading the data!

Some of the arguments available on rtweet are the following:
    query: Search query or queries e.g. "cat"
    exact_phrase: 	If TRUE, only tweets will be returned matching the exact phrase
    users: 	string or character vector, user handles to collect tweets from the specified users
    is_reply: 	If TRUE, only reply tweets will be returned
    is_verified: 	If TRUE, only tweets whose authors are verified by Twitter will be returned
    has_hashtags: 	If TRUE, only tweets containing hashtags will be returned
    has_links: 	If TRUE, only tweets containing links and media will be returned
    has_mentions: 	If TRUE, only tweets containing mentions will be returned
    has_geo: 	If TRUE, only tweets containing Tweet-specific geolocation data provided by the Twitter user will be returned
    place: 	Name of place e.g. "London"
    country: 	Name of country as ISO alpha-2 code e.g. "GB"
    point_radius: 	A vector of two point coordinates latitude, longitude, and point radius distance (in miles)
    bbox: 	A vector of four bounding box coordinates from west longitude to north latitude
    lang: 	A single BCP 47 language identifier e.g. "fr"
    url: 	string, return tweets containing specified url

Let's first build the query!

Useful tools to get bounding box coordinates include: 
      -https://www.openstreetmap.org/#map=12/33.7673/-84.4201
      -https://boundingbox.klokantech.com/

The alternative point_radius argument requires three pieces of information: the longitude and latitude of a target coordinate, and the buffer size around that coordinate.

Rest of the details on the query can also be specified in the call.
```{r}
build_query(
  country = "US",
  place = "atlanta",
  lang = "en",
  bbox= c(-84.4380, 33.7195, -84.3468, 33.8004),
  has_geo=T,
)
```

### Count number of tweets 
Now we check how many tweets we can access using this query:

```{r}
counts <- count_all_tweets(query = ("migrants has:geo"),
      start_tweets = "2020-03-21T00:00:00Z",
      end_tweets = "2020-03-28T01:00:00Z",
      bearer_token = get_bearer(),
      n = 50,
      granularity = "hour",     #Options are "day"; "hour"; "minute". Default is day
      verbose = TRUE,          #If FALSE, query progress messages are suppressed
      )

sum(counts$tweet_count)
```

```{r}
library(tidyverse)
library(lubridate)
counts %>% mutate(start = ymd_hms(start)) %>% select(start, tweet_count) %>%
  ggplot(aes(x = start, y = tweet_count)) + geom_line() +
  xlab("Date") + ylab("Number of #migrants tweets")
```


### Get all tweets (v2)
We can then download all the tweets!
```{r}

tweets <- get_all_tweets(
                query = c("migrants, has:geo"),
                start_tweets = "2020-03-21T00:00:00Z",
                end_tweets = "2020-03-28T01:00:00Z",
                n = 50,
                data_path=path_dir,
                bind_tweets= T,      #If TRUE, tweets captured are bound into a data.frame for assignment
                # user = T,          #we want to bundle user-level data,
                export_query=F       #If TRUE, queries are exported to data_path
              )

tweets
```

We have both users' profile and tweet information stored.
### Load data
```{r}
files <- list.files(path_dir, pattern = "^users")
user_content <- jsonlite::read_json(file.path(path_dir, files[1]), simplifyVector = T)
places_content <- user_content$users$location
length(places_content)
user_content$tweets$text
```


## We can also download other information:
### Tweets of a user

To extract tweets based on the user ID:
```{r user tweets}
users<- c("PopAssocAmerica")  

# stores tweets in data_path folder as a series of jsons
tweets_paa  <- get_all_tweets(users,
                          start_tweets = "2021-03-28T00:00:00Z",
                          end_tweets = "2022-03-06T11:59:59Z",
                          data_path = path_dir,                       #link to storage folder
                          bind_tweets = T,                        #convenience function to bundle the JSONs into a data.frame object
                          n=50,
                          bearer_token = get_bearer(),
                          )

tweets_paa
```


### Timeline (home and user-timeline) of a user

```{r timeline}
timeline_tweets <- get_user_timeline("2836918220",   #PopAssocAmerica requires user id
                        start_tweets = "2021-03-28T00:00:00Z",
                        end_tweets = "2022-03-06T11:59:59Z",
                        bearer_token = get_bearer(),
                        data_path = path_dir,                #link to storage folder
                        bind_tweets = T,                      #convenience function to bundle the JSONs into a data.frame object
                        n=50,
                        )
timeline_tweets
```


### Get list of followers
```{r followers}
users<- c("2836918220")  

followers <- get_user_followers(users)
length(followers$id)
followers
```

### Get list of friends
```{r friends}
friends <- get_user_following(users)
length(friends$id)
friends
```





## Exercises 
### When did @PopAssocAmerica create the account?
Check here for help: https://rdrr.io/cran/academictwitteR/man/get_user_profile.html
```{r}
```


### Find how many tweets related to #paa2022 have been sent out from @PopAssocAmerica this year
```{r}
```


### Find the top 5 languages used in the #migrant topic (with geo) this year 
```{r}
```


### Find how many friends you have in common with @PopAssocAmerica
```{r}
```

